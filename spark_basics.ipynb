{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark Basics Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x258a3dd7d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tom</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bill</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mars</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miles</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adore</td>\n",
       "      <td>42.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shiloh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name   age  experience    salary\n",
       "0     ben  23.0         8.0   30000.0\n",
       "1     tom  55.0        20.0    1500.0\n",
       "2    bill  16.0         2.0  100000.0\n",
       "3    mars  39.0        15.0    5000.0\n",
       "4   miles  25.0         9.0   12000.0\n",
       "5   adore  42.0        18.0   25000.0\n",
       "6  shiloh   NaN         NaN   40000.0\n",
       "7     NaN  34.0        10.0       NaN\n",
       "8     NaN  36.0         NaN       NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv('demo.csv')\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|   _c0| _c1|       _c2|   _c3|\n",
      "+------+----+----------+------+\n",
      "|  name| age|experience|salary|\n",
      "|   ben|  23|         8| 30000|\n",
      "|   tom|  55|        20|  1500|\n",
      "|  bill|  16|         2|100000|\n",
      "|  mars|  39|        15|  5000|\n",
      "| miles|  25|         9| 12000|\n",
      "| adore|  42|        18| 25000|\n",
      "|shiloh|null|      null| 40000|\n",
      "|  null|  34|        10|  null|\n",
      "|  null|  36|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string, experience: string, salary: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considers first row as header\n",
    "df_pyspark = spark.read.option('header','true').csv('demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  name| age|experience|salary|\n",
      "+------+----+----------+------+\n",
      "|   ben|  23|         8| 30000|\n",
      "|   tom|  55|        20|  1500|\n",
      "|  bill|  16|         2|100000|\n",
      "|  mars|  39|        15|  5000|\n",
      "| miles|  25|         9| 12000|\n",
      "| adore|  42|        18| 25000|\n",
      "|shiloh|null|      null| 40000|\n",
      "|  null|  34|        10|  null|\n",
      "|  null|  36|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='ben', age='23', experience='8', salary='30000')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='ben', age='23', experience='8', salary='30000'),\n",
       " Row(name='tom', age='55', experience='20', salary='1500'),\n",
       " Row(name='bill', age='16', experience='2', salary='100000')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similarto df.info()\n",
    "#everything is a string\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto detect dtype\n",
    "df_pyspark = spark.read.option('header','true').csv('demo.csv', inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  name| age|experience|salary|\n",
      "+------+----+----------+------+\n",
      "|   ben|  23|         8| 30000|\n",
      "|   tom|  55|        20|  1500|\n",
      "|  bill|  16|         2|100000|\n",
      "|  mars|  39|        15|  5000|\n",
      "| miles|  25|         9| 12000|\n",
      "| adore|  42|        18| 25000|\n",
      "|shiloh|null|      null| 40000|\n",
      "|  null|  34|        10|  null|\n",
      "|  null|  36|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternate method\n",
    "df_pyspark = spark.read.csv('demo.csv', header= True, inferSchema= True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience', 'salary']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return type is dataframe\n",
    "df_pyspark.select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|   ben|\n",
      "|   tom|\n",
      "|  bill|\n",
      "|  mars|\n",
      "| miles|\n",
      "| adore|\n",
      "|shiloh|\n",
      "|  null|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, experience: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['name','experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|experience|  name|\n",
      "+----------+------+\n",
      "|         8|   ben|\n",
      "|        20|   tom|\n",
      "|         2|  bill|\n",
      "|        15|  mars|\n",
      "|         9| miles|\n",
      "|        18| adore|\n",
      "|      null|shiloh|\n",
      "|        10|  null|\n",
      "|      null|  null|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['experience','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('experience', 'int'), ('salary', 'int')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, name: string, age: string, experience: string, salary: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+-----------------+\n",
      "|summary| name|               age|        experience|           salary|\n",
      "+-------+-----+------------------+------------------+-----------------+\n",
      "|  count|    7|                 8|                 7|                7|\n",
      "|   mean| null|             33.75|11.714285714285714|          30500.0|\n",
      "| stddev| null|12.302729081677075|  6.29058253037257|33626.62635472075|\n",
      "|    min|adore|                16|                 2|             1500|\n",
      "|    max|  tom|                55|                20|           100000|\n",
      "+-------+-----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, experience: int, salary: int, experience after 2 years: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column\n",
    "df_pyspark = df_pyspark.withColumn('experience after 2 years', df_pyspark['experience']+2)\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+------------------------+\n",
      "|  name| age|experience|salary|experience after 2 years|\n",
      "+------+----+----------+------+------------------------+\n",
      "|   ben|  23|         8| 30000|                      10|\n",
      "|   tom|  55|        20|  1500|                      22|\n",
      "|  bill|  16|         2|100000|                       4|\n",
      "|  mars|  39|        15|  5000|                      17|\n",
      "| miles|  25|         9| 12000|                      11|\n",
      "| adore|  42|        18| 25000|                      20|\n",
      "|shiloh|null|      null| 40000|                    null|\n",
      "|  null|  34|        10|  null|                      12|\n",
      "|  null|  36|      null|  null|                    null|\n",
      "+------+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, experience: int, salary: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop col\n",
    "df_pyspark.drop('experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+------------------------+\n",
      "|  name| age|experience|salary|experience after 2 years|\n",
      "+------+----+----------+------+------------------------+\n",
      "|   ben|  23|         8| 30000|                      10|\n",
      "|   tom|  55|        20|  1500|                      22|\n",
      "|  bill|  16|         2|100000|                       4|\n",
      "|  mars|  39|        15|  5000|                      17|\n",
      "| miles|  25|         9| 12000|                      11|\n",
      "| adore|  42|        18| 25000|                      20|\n",
      "|shiloh|null|      null| 40000|                    null|\n",
      "|  null|  34|        10|  null|                      12|\n",
      "|  null|  36|      null|  null|                    null|\n",
      "+------+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  name| age|experience|salary|\n",
      "+------+----+----------+------+\n",
      "|   ben|  23|         8| 30000|\n",
      "|   tom|  55|        20|  1500|\n",
      "|  bill|  16|         2|100000|\n",
      "|  mars|  39|        15|  5000|\n",
      "| miles|  25|         9| 12000|\n",
      "| adore|  42|        18| 25000|\n",
      "|shiloh|null|      null| 40000|\n",
      "|  null|  34|        10|  null|\n",
      "|  null|  36|      null|  null|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop column\n",
    "df_pyspark = df_pyspark.drop('experience after 2 years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|      null|  34|        10|  null|\n",
      "|      null|  36|      null|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename col\n",
    "df_pyspark = df_pyspark.withColumnRenamed('name','first name')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|first name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|       ben| 23|         8| 30000|\n",
      "|       tom| 55|        20|  1500|\n",
      "|      bill| 16|         2|100000|\n",
      "|      mars| 39|        15|  5000|\n",
      "|     miles| 25|         9| 12000|\n",
      "|     adore| 42|        18| 25000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop any row having a null\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to drop all rows that are null in every column\n",
    "#df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|first name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|       ben| 23|         8| 30000|\n",
      "|       tom| 55|        20|  1500|\n",
      "|      bill| 16|         2|100000|\n",
      "|      mars| 39|        15|  5000|\n",
      "|     miles| 25|         9| 12000|\n",
      "|     adore| 42|        18| 25000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop any row that has a null in it\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|      null|  34|        10|  null|\n",
      "|      null|  36|      null|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|      null|  34|        10|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete rows where Non null values are < thresholg\n",
    "# we want to keep rows with atleast 2 non null values\n",
    "df_pyspark.na.drop(how='any', thresh=2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|      null|  34|        10|  null|\n",
      "|      null|  36|      null|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any', thresh=1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|first name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|       ben| 23|         8| 30000|\n",
      "|       tom| 55|        20|  1500|\n",
      "|      bill| 16|         2|100000|\n",
      "|      mars| 39|        15|  5000|\n",
      "|     miles| 25|         9| 12000|\n",
      "|     adore| 42|        18| 25000|\n",
      "|      null| 34|        10|  null|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null values from a specific column\n",
    "df_pyspark.na.drop(how='any', subset= ['experience']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fillin missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H8> FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|       n/a|  34|        10|  null|\n",
      "|       n/a|  36|      null|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace all null values where col is string\n",
    "# only use for strings\n",
    "df_pyspark.na.fill('n/a').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|first name| age|experience|salary|\n",
      "+----------+----+----------+------+\n",
      "|       ben|  23|         8| 30000|\n",
      "|       tom|  55|        20|  1500|\n",
      "|      bill|  16|         2|100000|\n",
      "|      mars|  39|        15|  5000|\n",
      "|     miles|  25|         9| 12000|\n",
      "|     adore|  42|        18| 25000|\n",
      "|    shiloh|null|      null| 40000|\n",
      "|      null|  34|        10|  null|\n",
      "|      null|  36|      null|  null|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill single col\n",
    "df_pyspark.na.fill('22', 'experience').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H8> IMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = mean, median, mode\n",
    "imputer = Imputer(\n",
    "    inputCols=['age','experience','salary'],\n",
    "    outputCols = ['{}_imputed'.format(c) for c in ['age','experience','salary']]\n",
    "    ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|first name| age|experience|salary|age_imputed|experience_imputed|salary_imputed|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|       ben|  23|         8| 30000|         23|                 8|         30000|\n",
      "|       tom|  55|        20|  1500|         55|                20|          1500|\n",
      "|      bill|  16|         2|100000|         16|                 2|        100000|\n",
      "|      mars|  39|        15|  5000|         39|                15|          5000|\n",
      "|     miles|  25|         9| 12000|         25|                 9|         12000|\n",
      "|     adore|  42|        18| 25000|         42|                18|         25000|\n",
      "|    shiloh|null|      null| 40000|         33|                11|         40000|\n",
      "|      null|  34|        10|  null|         34|                10|         30500|\n",
      "|      null|  36|      null|  null|         36|                11|         30500|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|   ben| 23|         8| 30000|\n",
      "|   tom| 55|        20|  1500|\n",
      "|  bill| 16|         2|100000|\n",
      "|  mars| 39|        15|  5000|\n",
      "| miles| 25|         9| 12000|\n",
      "| adore| 42|        18| 25000|\n",
      "|shiloh| 34|        10| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('demo2.csv',header=True, inferSchema= True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|   ben| 23|         8| 30000|\n",
      "|  bill| 16|         2|100000|\n",
      "| adore| 42|        18| 25000|\n",
      "|shiloh| 34|        10| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('salary>=15000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|   ben| 23|\n",
      "|  bill| 16|\n",
      "| adore| 42|\n",
      "|shiloh| 34|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('salary>=15000').select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|   ben| 23|         8| 30000|\n",
      "|  bill| 16|         2|100000|\n",
      "| adore| 42|        18| 25000|\n",
      "|shiloh| 34|        10| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['salary'] >= 15000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|adore| 42|        18| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple conditions\n",
    "df_pyspark.filter((df_pyspark['salary'] >= 15000) & \n",
    "                  (df_pyspark['salary'] <= 27000)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|   ben| 23|         8| 30000|\n",
      "|  bill| 16|         2|100000|\n",
      "| adore| 42|        18| 25000|\n",
      "|shiloh| 34|        10| 40000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['salary'] >= 15000) | (df_pyspark['salary'] >= 15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|  tom| 55|        20|  1500|\n",
      "| mars| 39|        15|  5000|\n",
      "|miles| 25|         9| 12000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not (~)\n",
    "df_pyspark.filter(~(df_pyspark['salary'] >= 15000)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby & Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+\n",
      "|  name|    department|salary|\n",
      "+------+--------------+------+\n",
      "| miles|  data science| 30000|\n",
      "|shiloh|cloud engineer| 85000|\n",
      "| adore|  data science|100000|\n",
      "|shiloh|           iot|  5000|\n",
      "| miles|           iot| 12000|\n",
      "| adore|  data science| 25000|\n",
      "|shiloh|cloud engineer| 40000|\n",
      "+------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('demo3.csv', header= True, inferSchema= True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, sum(salary): bigint]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grou\n",
    "df_pyspark.groupby(df_pyspark['name']).sum()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|sum(salary)|\n",
      "+------+-----------+\n",
      "| adore|     125000|\n",
      "|shiloh|     130000|\n",
      "| miles|      42000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby(df_pyspark['name']).sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|max(salary)|\n",
      "+------+-----------+\n",
      "| adore|     100000|\n",
      "|shiloh|      85000|\n",
      "| miles|      30000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(df_pyspark['name']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|    department|       avg(salary)|\n",
      "+--------------+------------------+\n",
      "|  data science|51666.666666666664|\n",
      "|           iot|            8500.0|\n",
      "|cloud engineer|           62500.0|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(df_pyspark['department']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    department|count|\n",
      "+--------------+-----+\n",
      "|  data science|    3|\n",
      "|           iot|    2|\n",
      "|cloud engineer|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(df_pyspark['department']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|     297000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'salary' : 'sum'}).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|experience|salary|\n",
      "+------+---+----------+------+\n",
      "|   ben| 31|        10| 30000|\n",
      "|   tom| 30|         8| 25000|\n",
      "|  bill| 29|         4| 20000|\n",
      "|  mars| 24|         3| 20000|\n",
      "| adore| 21|         1| 15000|\n",
      "|shiloh| 23|         2| 18000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict salary using age & experience\n",
    "df_pyspark = spark.read.csv('demo4.csv', header= True, inferSchema= True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience', 'salary']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent features\n",
    "# age, experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group independent features into 1 col\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler = VectorAssembler(inputCols=['age','experience'], outputCol='independent features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+--------------------+\n",
      "|  name|age|experience|salary|independent features|\n",
      "+------+---+----------+------+--------------------+\n",
      "|   ben| 31|        10| 30000|         [31.0,10.0]|\n",
      "|   tom| 30|         8| 25000|          [30.0,8.0]|\n",
      "|  bill| 29|         4| 20000|          [29.0,4.0]|\n",
      "|  mars| 24|         3| 20000|          [24.0,3.0]|\n",
      "| adore| 21|         1| 15000|          [21.0,1.0]|\n",
      "|shiloh| 23|         2| 18000|          [23.0,2.0]|\n",
      "+------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|independent features|salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalised_data = output.select('independent features','salary')\n",
    "finalised_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "train_data, test_data = finalised_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol= 'independent features', labelCol= 'salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-714.2857, 3485.7143])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26857.142857139796"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+\n",
      "|independent features|salary|       prediction|\n",
      "+--------------------+------+-----------------+\n",
      "|          [30.0,8.0]| 25000|33314.28571428435|\n",
      "|         [31.0,10.0]| 30000|39571.42857142653|\n",
      "+--------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8942.857142855439, 80369795.91833645)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
